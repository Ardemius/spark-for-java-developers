= Apache Spark for Java developers
Thomas SCHWENDER <https://github.com/ardemius[@ardemius]>
// Handling GitHub admonition blocks icons
ifndef::env-github[:icons: font]
ifdef::env-github[]
:status:
:outfilesuffix: .adoc
:caution-caption: :fire:
:important-caption: :exclamation:
:note-caption: :paperclip:
:tip-caption: :bulb:
:warning-caption: :warning:
endif::[]
:imagesdir: ./images
:source-highlighter: highlightjs
// Next 2 ones are to handle line breaks in some particular elements (list, footnotes, etc.)
:lb: pass:[<br> +]
:sb: pass:[<br>]
// check https://github.com/Ardemius/personal-wiki/wiki/AsciiDoctor-tips for tips on table of content in GitHub
:toc: macro
:toclevels: 2
// To turn off figure caption labels and numbers
:figure-caption!:

toc::[]

Udemy training : https://www.udemy.com/course/apache-spark-for-java-developers

== 1 - Introduction

=== 1.3 - Introduction

*Hadoop* est un modèle rigide : on fait obligatoirement 1 *map* suivi d'1 *reduce*, et, à la fin de cet enchaînement, le tout est obligatoirement écrit sur disque. +
En comparaison, Spark propose plus de 80 high-level operators, à la place des 2 seuls map et reduce.

Avec Spark, en *standalone mode* (single computer), même pour de petits datasets, on bénéficie totalement du *multi-core parallel processing* (le multi-threading est géré par Spark, pas besoin de le gérer soi-même en Java, ce qui est toujours considéré comme compliqué). +
-> Pas besoin de se trouver dans des conditions "Big Data" pour trouver un intérêt à Spark.

=== 1.4 - Spark architecture and RDDs

* A *partition* is a *block of data*, NOT a node.
* A *task* : a function executing against a partition
* *RDD* : Resilient Distributed Dataset
* The Spark *DAG* is an *execution plan*

== 2 - Getting Started

.Spark doesn't work with Java 9
[NOTE]
====
Please note at the time of writing (November 2018), Spark does not support Java 9 onwards. +
This is due to their internal use of a class called sun.misc.Unsafe which has been removed from Java 9.
====

."An illegal reflective access operation has occurred" exception
[NOTE]
====
To avoid the _"An illegal reflective access operation has occurred"_ when running Spark program in IntelliJ:

----
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.hadoop.security.authentication.util.KerberosUtil (file:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.9.0.jar) to method sun.security.krb5.Config.getInstance()
WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.security.authentication.util.KerberosUtil
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
----

Be sure to use Java 8, and not higher. +
This has to be configured in _"Project Structure"_ menu.
====

== 6 - PairRDDs

=== 6.15 - Building a PairRDD

Même si un *PairRDD* ressemble beaucoup à une Map en Java, contrairement à cette dernière, le PairRDD permet d'avoir de *multiples instances de la même key*.

Pour utiliser un PairRDD et un Tuple2 ? Tout simplement parce que le PairRDD dispose de méthodes que le Tuple2 n'a pas.

=== 6.16 - Coding a ReduceByKey

[WARNING]
====
*Group by key* method can lead to severe (catastrophic) performance problems! +
Avoid unless you're sure there's no better alternative
====

This is due to the underlying `PairRDD <String, Iterable<String>>`. +
`Iterable` doesn't contain many convenient methods.

-> It is far better to use a *Reduce by key* (`pairRDD.reduceByKey`)

== 8 - Reading from Disk

== 8.21 - Reading from Disk

By reading a file with `JavaSparkContext.textFile`, we encounter the following issue:

----
19/12/02 18:05:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
19/12/02 18:05:24 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:278)
----

This comes from Windows lacking Hadoop librairies, which are looked for by the `textFile` method. +
To avoid this issue, we have to add a `winutils.exe` as an environment variables. +
This .exe file contains the missing Hadoop libraries for Windows. +
Here is the command to do so:

----
System.setProperty("hadoop.home.dir", "c:/path_to/hadoop")
----

We can retrieve it from Steve Loughran GitHub repo : https://github.com/steveloughran/winutils

NOTE: As Steve explains, it doesn't have the time to maintain the project, so for the lastest Hadoop version, have a look at this other GitHub repo : https://github.com/cdarlint/winutils

== 9 - Keyword Ranking Pratical

=== 9.22 - Practical Requirements

Exercice:

* load _subtitles/input.txt_ into a RDD
* get rid of all "boring" words (from _subtitles/boringwords.txt_)
* count remaining words and find the 10 most used

== 10 - Sorts and Coalesce

=== 10.26 - Why Coalesce is the wrong solution?

WARNING: `foreach` executes the lambda on each partition *in parallel*.

-> It means that several threads will compete to execute the lambda, which is NOT compatible with a correct *sort* result. +
Example:

* 1st thread prints "1, 2, 3"
* then 2nd thread prints "1, 2"
* then 3rd thread prints "1, 2, 3, 4, 5"
* which gives a final result of "1, 2, 3, 1, 2, 1 ,2 ,3 ,4, 5"

-> NOT SORTED

image::spark-training_01.jpg[]

A solution is to call a prior `take` method (by example) that is aware of the different partitions being processed, and will retrieve the elements in the good order for a next coming display.

----
JavaPairRDD<Long, String> sorted = switched.sortByKey(false);

int numPartitions = sorted.getNumPartitions();
System.out.println("Number of partitions at this level: " + numPartitions);
// This foreach will NOT give a correct sort, as its associated lambda is going to be executed in parallel by multiple threads
// sorted.foreach(element -> System.out.println(element));

List<Tuple2<Long,String>> results = sorted.take(10);
results.forEach(System.out::println);
----

=== 10.27 - What is Coalesce used for in Spark

.coalesce correct usage
image::spark-training_02.jpg[]

* Coalesce is used for *performance reasons*, never for correctness.

.collect correct usage
image::spark-training_03.jpg[]

.The truth about shuffling data and knowing about partitions
image::spark-training_04.jpg[]

== 11 - Deployint to AWS EMR

*EMR* : *Elastic Map Reduce*, which is Amazon implementation of Hadoop in the cloud.

=== 11.30 - Running Spark Job on EMR

*Spark history server* sur le port 18080.

=== 11.31 - Understanding the Job Progress Output

image::spark-training_05.jpg[]
image::spark-training_06.jpg[]

----
[Stage 0:=====================>         (X + Y) / Z]
----

* 1st number (X) : number of tasks that have completed
* 2nd number (Y) : number of tasks that are actually running
* 3rd number (Z) : number of tasks that need to be run

.Reminder about what is a task
NOTE: A task is nothing more than a set of code that is executed against a partition

At the time of recording, the default size of a partition was 64 Mo on S3 (perhaps different at 2019/11, check `spark.files.maxPartitionBytes` configuration parameter ???). +
The input file process in this example was \~2.8 Go, so:

* 46 x 64 = 2944 Mo / 1024 = 2,875 Go
* Here we have 2 executors nodes, each of them has 4 cores, so for the 8 tasks actually running.

== 14 - RDD Performance

=== 14.48 - Transformations and actions

Let's take example from lecture 22 "Keyword Ranking Practical":

[source, java]
----
SparkConf conf = new SparkConf().setAppName("startingSpark").setMaster("local[*]");

try (JavaSparkContext sc = new JavaSparkContext(conf)) {

    JavaRDD<String> initialRdd = sc.textFile("src/main/resources/subtitles/input.txt");

    JavaRDD<String> lettersOnlyRdd = initialRdd.map( sentence -> sentence.replaceAll("[^a-zA-Z\\s]", "").toLowerCase() );

    JavaRDD<String> removedBlankLines = lettersOnlyRdd.filter( sentence -> sentence.trim().length() > 0 );

    JavaRDD<String> justWords = removedBlankLines.flatMap(sentence -> Arrays.asList(sentence.split(" ")).iterator());

    JavaRDD<String> blankWordsRemoved = justWords.filter(word -> word.trim().length() > 0);

    JavaRDD<String> justInterestingWords = blankWordsRemoved.filter(word -> Util.isNotBoring(word));

    JavaPairRDD<String, Long> pairRdd = justInterestingWords.mapToPair(word -> new Tuple2<String, Long>(word, 1L));

    JavaPairRDD<String, Long> totals = pairRdd.reduceByKey((value1, value2) -> value1 + value2);

    JavaPairRDD<Long, String> switched = totals.mapToPair(tuple -> new Tuple2<Long, String> (tuple._2, tuple._1 ));

    JavaPairRDD<Long, String> sorted = switched.sortByKey(false);

    int numPartitions = sorted.getNumPartitions();
    System.out.println("Number of partitions at this level: " + numPartitions);
    // This foreach will NOT give a correct sort, as its associated lambda is going to be executed in parallel by multiple threads
    //sorted.foreach(element -> System.out.println(element));

	// Here is the Spark "action", all preceding lines were only building an execution plan
    List<Tuple2<Long,String>> results = sorted.take(10);

    results.forEach(System.out::println);
}
----

[IMPORTANT]
====
We were told in the introduction that nearly each line was creating a new RDD, which is not true.
In fact, *until a Spark operation is performed* (meaning when Spark has to do a calculation to provide a result, which is named an *action*), everything that we are doing are only *transformations*, which are "just" building an *execution plan*.
====

* *Transformations* are lazily executed, only when an action is reached. +
Check the list in Spark Official doc here: https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations[RDD programming guide: transformations]
* *Actions* will result in the execution plan becoming an execution. +
Check the list in Spark Official doc here: https://spark.apache.org/docs/latest/rdd-programming-guide.html#actions[RDD programming guide: actions]

=== 14.49 - The DAG and Spark UI

IMPORTANT: The *execution plan* is a *DAG* (Directed Acyclic Graph), meaning a graph that doesn't have any loop inside.

.Hack to display the execution plan without installing Spark
[TIP]
====
A hack to print the DAG without having to install Spark: when a Spark program is running, Spark starts a webserver on port 4040 (meaning accessible through http://localhost:4040)

image:spark-training_07.jpg[]
image:spark-training_08.jpg[]
====

=== 14.50 - Narrow vs Wide Transformations

* *Narrow transformation*: Spark can implement the transformation without moving any data around (from a partition to another), hence its name "narrow" transformation.
* Whereas an operation like `rdd.groupByKey()` is a *wide transformation*. +
It can only be performed by serializing and copying data between partitions (a *shuffle*), implying a *great I/O cost*.

.Example of `rdd.groupByKey()` wide transformation and its associated shuffles
image::spark-training_09.jpg[]

=== 14.51 - Shuffles

IMPORTANT: Generally a Spark *stage* is a series of transformations that *don't need a shuffle*.

When Spark needs a shuffle, it creates a new stage, as shown here: +
image:spark-training_10.jpg[]

-> In the previous DAG example, the shuffle happened after the map.

=== 14.52 - Dealing with key skews

* A *join* is a wide transformation

=== 15.53 - Avoiding groupByKey and using map-side-reduces instead

ReduceByKey can do a reduce on the partitions FIRST. +
This is a called a *Map Side Reduce*. +
-> It greatly reduces the amount of data shuffled.

In most cases, a `groupByKey` can be replaced by a `reduceByKey`. +
Whenever possible, avoid `groupByKey` which suffers from the following issues:

* it implies a very big shuffle, and so a great I/O cost.
* as we manipulate very big data sets, *`groupByKey` could result in storing To of data in memory*, which would result in an OOM (Out Of Memory) exception.

=== 22.66 - Multiple grouping

WARNING: You can only perform aggregation on column that you do NOT group.

In this example:

----
select level, month from logging_table group by level, month
----

You can't apply any aggregation function on either `level` or `month`. +
The "trick" to do so, is to add a new column on which to apply your function:

----
select level, month, count(1) from logging_table group by level, month
----

=== 23.67 - Ordering

WARNING: Any column that is not part of a "grouping", *MUST* have an aggregation function performed on it.

=== 24.69 - DataFrame grouping

TIP: For a good explanation of what is a SQL *roll-up*, check http://www.sqltutorial.org/sql-rollup/



