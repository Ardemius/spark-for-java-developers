= Apache Spark for Java developers
Thomas SCHWENDER <https://github.com/ardemius[@ardemius]>
// Handling GitHub admonition blocks icons
ifndef::env-github[:icons: font]
ifdef::env-github[]
:status:
:outfilesuffix: .adoc
:caution-caption: :fire:
:important-caption: :exclamation:
:note-caption: :paperclip:
:tip-caption: :bulb:
:warning-caption: :warning:
endif::[]
:imagesdir: ./images
:source-highlighter: highlightjs
// Next 2 ones are to handle line breaks in some particular elements (list, footnotes, etc.)
:lb: pass:[<br> +]
:sb: pass:[<br>]
// check https://github.com/Ardemius/personal-wiki/wiki/AsciiDoctor-tips for tips on table of content in GitHub
:toc: macro
:toclevels: 2
// To turn off figure caption labels and numbers
:figure-caption!:

toc::[]

Udemy training : https://www.udemy.com/course/apache-spark-for-java-developers

== 1 - Introduction

=== 1.3 - Introduction

*Hadoop* est un modèle rigide : on fait obligatoirement 1 *map* suivi d'1 *reduce*, et, à la fin de cet enchaînement, le tout est obligatoirement écrit sur disque. +
En comparaison, Spark propose plus de 80 high-level operators, à la place des 2 seuls map et reduce.

Avec Spark, en *standalone mode* (single computer), même pour de petits datasets, on bénéficie totalement du *multi-core parallel processing* (le multi-threading est géré par Spark, pas besoin de le gérer soi-même en Java, ce qui est toujours considéré comme compliqué). +
-> Pas besoin de se trouver dans des conditions "Big Data" pour trouver un intérêt à Spark.

=== 1.4 - Spark architecture and RDDs

* A *partition* is a *block of data*, NOT a node.
* A *task* : a function executing against a partition
* *RDD* : Resilient Distributed Dataset
* The Spark *DAG* is an *execution plan*

== 2 - Getting Started

.Spark doesn't work with Java 9
[NOTE]
====
Please note at the time of writing (November 2018), Spark does not support Java 9 onwards. +
This is due to their internal use of a class called sun.misc.Unsafe which has been removed from Java 9.
====

== 6 - PairRDDs

=== 6.15 - Building a PairRDD

Même si un *PairRDD* ressemble beaucoup à une Map en Java, contrairement à cette dernière, le PairRDD permet d'avoir de *multiples instances de la même key*.

Pour utiliser un PairRDD et un Tuple2 ? Tout simplement parce que le PairRDD dispose de méthodes que le Tuple2 n'a pas.

=== 6.16 - Coding a ReduceByKey

[WARNING]
====
*Group by key* method can lead to severe (catastrophic) performance problems! +
Avoid unless you're sure there's no better alternative
====

This is due to the underlying `PairRDD <String, Iterable<String>>`. +
`Iterable` doesn't contain many convenient methods.

-> It is far better to use a *Reduce by key* (`pairRDD.reduceByKey`)

== 8 - Reading from Disk

== 8.21 - Reading from Disk

By reading a file with `JavaSparkContext.textFile`, we encounter the following issue:

----
19/12/02 18:05:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
19/12/02 18:05:24 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:278)
----

This comes from Windows lacking Hadoop librairies, which are looked for by the `textFile` method. +
To avoid this issue, we have to add a `winutils.exe` as an environment variables. +
This .exe file contains the missing Hadoop libraries for Windows. +
Here is the command to do so:

----
System.setProperty("hadoop.home.dir", "c:/path_to/hadoop")
----

We can retrieve it from Steve Loughran GitHub repo : https://github.com/steveloughran/winutils

NOTE: As Steve explains, it doesn't have the time to maintain the project, so for the lastest Hadoop version, have a look at this other GitHub repo : https://github.com/cdarlint/winutils

== 9 - Keyword Ranking Pratical

=== 9.22 - Practical Requirements

Exercice:

* load _subtitles/input.txt_ into a RDD
* get rid of all "boring" words (from _subtitles/boringwords.txt_)
* count remaining words and find the 10 most used

== 10 - Sorts and Coalesce

=== 10.26 - Why Coalesce is the wrong solution?

WARNING: `foreach` executes the lambda on each partition *in parallel*.

-> It means that several threads will compete to execute the lambda, which is NOT compatible with a correct *sort* result. +
Example:

* 1st thread prints "1, 2, 3"
* then 2nd thread prints "1, 2"
* then 3rd thread prints "1, 2, 3, 4, 5"
* which gives a final result of "1, 2, 3, 1, 2, 1 ,2 ,3 ,4, 5"

-> NOT SORTED

image::spark-training_01.jpg[]

A solution is to call a prior `take` method (by example) that is aware of the different partitions being processed, and will retrieve the elements in the good order for a next coming display.

----
JavaPairRDD<Long, String> sorted = switched.sortByKey(false);

int numPartitions = sorted.getNumPartitions();
System.out.println("Number of partitions at this level: " + numPartitions);
// This foreach will NOT give a correct sort, as its associated lambda is going to be executed in parallel by multiple threads
// sorted.foreach(element -> System.out.println(element));

List<Tuple2<Long,String>> results = sorted.take(10);
results.forEach(System.out::println);
----

=== 10.27 - What is Coalesce used for in Spark

.coalesce correct usage
image::spark-training_02.jpg[]

* Coalesce is used for *performance reasons*, never for correctness.

.collect correct usage
image::spark-training_03.jpg[]

.The truth about shuffling data and knowing about partitions
image::spark-training_04.jpg[]

== 11 - Deployint to AWS EMR

*EMR* : *Elastic Map Reduce*, which is Amazon implementation of Hadoop in the cloud.

=== 11.30 - Running Spark Job on EMR

*Spark history server* sur le port 18080.

=== 11.31 - Understanding the Job Progress Output

image::spark-training_05.jpg[]
image::spark-training_06.jpg[]

----
[Stage 0:=====================>         (X + Y) / Z]
----

* 1st number (X) : number of tasks that have completed
* 2nd number (Y) : number of tasks that are actually running
* 3rd number (Z) : number of tasks that need to be run

.Reminder about what is a task
NOTE: A task is nothing more than a set of code that is executed against a partition

At the time of recording, the default size of a partition was 64 Mo (should be 128 Mo at 2019/11, chez `spark.files.maxPartitionBytes` configuration parameter). +
The input file process in this example was \~2.8 Go, so:

* 46 x 64 = 2944 Mo / 1024 = 2,875 Go
* Here we have 2 executors nodes, each of them has 4 cores, so for the 8 tasks actually running.









