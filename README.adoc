= Apache Spark for Java developers
Thomas SCHWENDER <https://github.com/ardemius[@ardemius]>
// Handling GitHub admonition blocks icons
ifndef::env-github[:icons: font]
ifdef::env-github[]
:status:
:outfilesuffix: .adoc
:caution-caption: :fire:
:important-caption: :exclamation:
:note-caption: :paperclip:
:tip-caption: :bulb:
:warning-caption: :warning:
endif::[]
:imagesdir: ./images
:source-highlighter: highlightjs
// Next 2 ones are to handle line breaks in some particular elements (list, footnotes, etc.)
:lb: pass:[<br> +]
:sb: pass:[<br>]
// check https://github.com/Ardemius/personal-wiki/wiki/AsciiDoctor-tips for tips on table of content in GitHub
:toc: macro
:toclevels: 2
// To turn off figure caption labels and numbers
:figure-caption!:

toc::[]

Udemy training : https://www.udemy.com/course/apache-spark-for-java-developers

== 1 - Introduction

=== 1.3 - Introduction

*Hadoop* est un modèle rigide : on fait obligatoirement 1 *map* suivi d'1 *reduce*, et, à la fin de cet enchaînement, le tout est obligatoirement écrit sur disque. +
En comparaison, Spark propose plus de 80 high-level operators, à la place des 2 seuls map et reduce.

Avec Spark, en *standalone mode* (single computer), même pour de petits datasets, on bénéficie totalement du *multi-core parallel processing* (le multi-threading est géré par Spark, pas besoin de le gérer soi-même en Java, ce qui est toujours considéré comme compliqué). +
-> Pas besoin de se trouver dans des conditions "Big Data" pour trouver un intérêt à Spark.

=== 1.4 - Spark architecture and RDDs

* A *partition* is a *block of data*, NOT a node.
* A *task* : a function executing against a partition
* *RDD* : Resilient Distributed Dataset
* The Spark *DAG* is an *execution plan*

== 2 - Getting Started

.Spark doesn't work with Java 9
[NOTE]
====
Please note at the time of writing (November 2018), Spark does not support Java 9 onwards. +
This is due to their internal use of a class called sun.misc.Unsafe which has been removed from Java 9.
====

== 6 - PairRDDs

=== 6.15 - Building a PairRDD

Même si un *PairRDD* ressemble beaucoup à une Map en Java, contrairement à cette dernière, le PairRDD permet d'avoir de *multiples instances de la même key*.

Pour utiliser un PairRDD et un Tuple2 ? Tout simplement parce que le PairRDD dispose de méthodes que le Tuple2 n'a pas.

=== 6.16 - Coding a ReduceByKey

[WARNING]
====
*Group by key* method can lead to severe (catastrophic) performance problems! +
Avoid unless you're sure there's no better alternative
====

This is due to the underlying `PairRDD <String, Iterable<String>>`. +
`Iterable` doesn't contain many convenient methods.

-> It is far better to use a *Reduce by key* (`pairRDD.reduceByKey`)

== 8 - Reading from Disk

== 8.21 - Reading from Disk

By reading a file with `JavaSparkContext.textFile`, we encounter the following issue:

----
19/12/02 18:05:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
19/12/02 18:05:24 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:278)
----

This comes from Windows lacking Hadoop librairies, which are looked for by the `textFile` method. +
To avoid this issue, we have to add a `winutils.exe` as an environment variables. +
This .exe file contains the missing Hadoop libraries for Windows. +
Here is the command to do so:

----
System.setProperty("hadoop.home.dir", "c:/path_to/hadoop")
----

We can retrieve it from Steve Loughran GitHub repo : https://github.com/steveloughran/winutils

NOTE: As Steve explains, it doesn't have the time to maintain the project, so for the lastest Hadoop version, have a look at this other GitHub repo : https://github.com/cdarlint/winutils

== 9 - Keyword Ranking Pratical

=== 9.22 - Practical Requirements

Exercice:

* load _subtitles/input.txt_ into a RDD
* get rid of all "boring" words (from _subtitles/boringwords.txt_)
* count remaining words and find the 10 most used

